# query.py
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

# -----------------------------
# 1Ô∏è‚É£ Setup embeddings
# -----------------------------
embedding_function = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# -----------------------------
# 2Ô∏è‚É£ Setup vector database
# -----------------------------
vectordb = Chroma(
    collection_name="my_collection",
    embedding_function=embedding_function,
    persist_directory="./chroma_db"
)

# -----------------------------
# 3Ô∏è‚É£ Setup LLM
# -----------------------------
llm = ChatOpenAI(
    model_name="gpt-4o-mini",
    temperature=0,
    openai_api_key="sk-"  # <-- Add your OpenAI API key
)

# -----------------------------
# 4Ô∏è‚É£ Setup Retrieval QA
# -----------------------------
qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectordb.as_retriever(),
    chain_type="stuff"
)

# -----------------------------
# 5Ô∏è‚É£ Main loop
# -----------------------------
print("üìò Ask me anything about your subjects (type 'exit' to quit)\n")

while True:
    query = input("Your question: ")
    if query.lower() in ["exit", "quit"]:
        print("Bye!")
        break
    result = qa.invoke(query)  # using invoke to avoid deprecated .run()
    print("\nAnswer:", result, "\n")
